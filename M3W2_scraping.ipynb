{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last week, we practiced importing various files into Python and opened them as a pandas DataFrame. Concretely, we saw how to import .txt files, .csv and .xlsx files and last but not least, relational databases. \n",
    "\n",
    "\n",
    "However, this means that we had these files locally. Often that is not the case. Going to a website and manually downloading/copying information there is not scalable and not reproducible. You'd likely need to do that via code. This is the topic of this workshop. We will practice:\n",
    "\n",
    "- Importing data from the web and loading it to pandas\n",
    "- Make https requests\n",
    "- Scrape and parse html code \n",
    "\n",
    "\n",
    "Some jargon, or what do some popular abbreviations mean\n",
    "\n",
    "- **URL**: Universal/Unifom resource locator\n",
    "- **HTTP**: HypterText Transfer Portal\n",
    "- **HTML**: HypterText Markup Language\n",
    "- **API**: Application Programming Interface\n",
    "- **JSON**: JavaScript Object Notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: BeautifulSoup4 in c:\\users\\mishe\\anaconda3\\lib\\site-packages (4.9.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\mishe\\anaconda3\\lib\\site-packages (from BeautifulSoup4) (2.0.1)\n",
      "Requirement already satisfied: MechanicalSoup in c:\\users\\mishe\\anaconda3\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0 in c:\\users\\mishe\\anaconda3\\lib\\site-packages (from MechanicalSoup) (2.24.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\mishe\\anaconda3\\lib\\site-packages (from MechanicalSoup) (4.5.2)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4 in c:\\users\\mishe\\anaconda3\\lib\\site-packages (from MechanicalSoup) (4.9.1)\n",
      "Requirement already satisfied: six>=1.4 in c:\\users\\mishe\\anaconda3\\lib\\site-packages (from MechanicalSoup) (1.15.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\mishe\\anaconda3\\lib\\site-packages (from requests>=2.0->MechanicalSoup) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\mishe\\anaconda3\\lib\\site-packages (from requests>=2.0->MechanicalSoup) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\mishe\\anaconda3\\lib\\site-packages (from requests>=2.0->MechanicalSoup) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mishe\\anaconda3\\lib\\site-packages (from requests>=2.0->MechanicalSoup) (2020.6.20)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\mishe\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.4->MechanicalSoup) (2.0.1)\n",
      "Requirement already satisfied: lxml in c:\\users\\mishe\\anaconda3\\lib\\site-packages (4.5.2)\n"
     ]
    }
   ],
   "source": [
    "# Packages you will need\n",
    "\n",
    "!pip install BeautifulSoup4\n",
    "!pip install MechanicalSoup\n",
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data from the web using the urllib module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with a popular data set, the so-called 'Wisconsin breast cancer' data, which is uploaded to the UCI Machine learning repository. The data is stored in one file ('breast-cancer-wisconsin.data'). However, it does NOT contain the column names. That information is stored in another file, called 'breast-cancer-wisconsin.names'. We will retrieve them both. \n",
    "\n",
    "\n",
    "Here is an example importing that data using the **urllib** module.\n",
    "\n",
    "\n",
    "We will download data from the ***UCI Machine learning Repository***. The parent directory can be found here: https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('breast-cancer-wisconsin.data', <http.client.HTTPMessage at 0x2237f143be0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "url_data = 'https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data'\n",
    "urlretrieve(url_data, 'breast-cancer-wisconsin.data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1. Using the same module and structure, retrieve the names of the breast cancer data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('breast-cancer-wisconsin.names', <http.client.HTTPMessage at 0x2237f143df0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_names = 'https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.names'\n",
    "urlretrieve(url_names, 'breast-cancer-wisconsin.names')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2. Import the 'breast-cancer-wisconsin.data' file as a pandas DataFrame. Print the top 5 rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000025</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002945</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1015425</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1016277</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1017023</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0   1   2   3   4   5   6   7   8   9   10\n",
       "0  1000025   5   1   1   1   2   1   3   1   1   2\n",
       "1  1002945   5   4   4   5   7  10   3   2   1   2\n",
       "2  1015425   3   1   1   1   2   2   3   1   1   2\n",
       "3  1016277   6   8   8   1   3   4   3   7   1   2\n",
       "4  1017023   4   1   1   3   2   1   3   1   1   2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data file in pandas, making sure there is no header here\n",
    "bc = pd.read_csv('breast-cancer-wisconsin.data', header=None)\n",
    "bc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3. So far so good. Sometimes it can be quite challenging to work with html files or broadly any non-well structured files we download from the web. Inspect the 'breast-cancer-wisconsin.names' in a text editor. As you can see, it is a long file and contains quite a lot of information, not all of which will be relevant for us. We will try to extract the relevant information in a bit.** \n",
    "\n",
    "    First, import the file, making sure you read it line by line and remove the new line character ('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the text file and read it line by line, removing the new line character\n",
    "\n",
    "lines = []\n",
    "with open('breast-cancer-wisconsin.names', mode='r') as file:\n",
    "    for line in file:\n",
    "        lines.append(line.strip('\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the output, which should look like the one below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Citation Request:',\n",
       " '   This breast cancer databases was obtained from the University of Wisconsin',\n",
       " '   Hospitals, Madison from Dr. William H. Wolberg.  If you publish results',\n",
       " '   when using this database, then please include this information in your',\n",
       " '   acknowledgements.  Also, please cite one or more of:',\n",
       " '',\n",
       " '   1. O. L. Mangasarian and W. H. Wolberg: \"Cancer diagnosis via linear ',\n",
       " '      programming\", SIAM News, Volume 23, Number 5, September 1990, pp 1 & 18.',\n",
       " '',\n",
       " '   2. William H. Wolberg and O.L. Mangasarian: \"Multisurface method of ',\n",
       " '      pattern separation for medical diagnosis applied to breast cytology\", ',\n",
       " '      Proceedings of the National Academy of Sciences, U.S.A., Volume 87, ',\n",
       " '      December 1990, pp 9193-9196.',\n",
       " '',\n",
       " '   3. O. L. Mangasarian, R. Setiono, and W.H. Wolberg: \"Pattern recognition ',\n",
       " '      via linear programming: Theory and application to medical diagnosis\", ',\n",
       " '      in: \"Large-scale numerical optimization\", Thomas F. Coleman and Yuying',\n",
       " '      Li, editors, SIAM Publications, Philadelphia 1990, pp 22-30.',\n",
       " '',\n",
       " '   4. K. P. Bennett & O. L. Mangasarian: \"Robust linear programming ',\n",
       " '      discrimination of two linearly inseparable sets\", Optimization Methods',\n",
       " '      and Software 1, 1992, 23-34 (Gordon & Breach Science Publishers).',\n",
       " '',\n",
       " '1. Title: Wisconsin Breast Cancer Database (January 8, 1991)',\n",
       " '',\n",
       " '2. Sources:',\n",
       " '   -- Dr. WIlliam H. Wolberg (physician)',\n",
       " '      University of Wisconsin Hospitals',\n",
       " '      Madison, Wisconsin',\n",
       " '      USA',\n",
       " '   -- Donor: Olvi Mangasarian (mangasarian@cs.wisc.edu)',\n",
       " '      Received by David W. Aha (aha@cs.jhu.edu)',\n",
       " '   -- Date: 15 July 1992',\n",
       " '',\n",
       " '3. Past Usage:',\n",
       " '',\n",
       " '   Attributes 2 through 10 have been used to represent instances.',\n",
       " '   Each instance has one of 2 possible classes: benign or malignant.',\n",
       " '',\n",
       " '   1. Wolberg,~W.~H., \\\\& Mangasarian,~O.~L. (1990). Multisurface method of ',\n",
       " '      pattern separation for medical diagnosis applied to breast cytology. In',\n",
       " '      {\\\\it Proceedings of the National Academy of Sciences}, {\\\\it 87},',\n",
       " '      9193--9196.',\n",
       " '      -- Size of data set: only 369 instances (at that point in time)',\n",
       " '      -- Collected classification results: 1 trial only',\n",
       " '      -- Two pairs of parallel hyperplanes were found to be consistent with',\n",
       " '         50% of the data',\n",
       " '         -- Accuracy on remaining 50% of dataset: 93.5%',\n",
       " '      -- Three pairs of parallel hyperplanes were found to be consistent with',\n",
       " '         67% of data',\n",
       " '         -- Accuracy on remaining 33% of dataset: 95.9%',\n",
       " '',\n",
       " '   2. Zhang,~J. (1992). Selecting typical instances in instance-based',\n",
       " '      learning.  In {\\\\it Proceedings of the Ninth International Machine',\n",
       " '      Learning Conference} (pp. 470--479).  Aberdeen, Scotland: Morgan',\n",
       " '      Kaufmann.',\n",
       " '      -- Size of data set: only 369 instances (at that point in time)',\n",
       " '      -- Applied 4 instance-based learning algorithms ',\n",
       " '      -- Collected classification results averaged over 10 trials',\n",
       " '      -- Best accuracy result: ',\n",
       " '         -- 1-nearest neighbor: 93.7%',\n",
       " '         -- trained on 200 instances, tested on the other 169',\n",
       " '      -- Also of interest:',\n",
       " '         -- Using only typical instances: 92.2% (storing only 23.1 instances)',\n",
       " '         -- trained on 200 instances, tested on the other 169',\n",
       " '',\n",
       " '4. Relevant Information:',\n",
       " '',\n",
       " '   Samples arrive periodically as Dr. Wolberg reports his clinical cases.',\n",
       " '   The database therefore reflects this chronological grouping of the data.',\n",
       " '   This grouping information appears immediately below, having been removed',\n",
       " '   from the data itself:',\n",
       " '',\n",
       " '     Group 1: 367 instances (January 1989)',\n",
       " '     Group 2:  70 instances (October 1989)',\n",
       " '     Group 3:  31 instances (February 1990)',\n",
       " '     Group 4:  17 instances (April 1990)',\n",
       " '     Group 5:  48 instances (August 1990)',\n",
       " '     Group 6:  49 instances (Updated January 1991)',\n",
       " '     Group 7:  31 instances (June 1991)',\n",
       " '     Group 8:  86 instances (November 1991)',\n",
       " '     -----------------------------------------',\n",
       " '     Total:   699 points (as of the donated datbase on 15 July 1992)',\n",
       " '',\n",
       " '   Note that the results summarized above in Past Usage refer to a dataset',\n",
       " '   of size 369, while Group 1 has only 367 instances.  This is because it',\n",
       " '   originally contained 369 instances; 2 were removed.  The following',\n",
       " \"   statements summarizes changes to the original Group 1's set of data:\",\n",
       " '',\n",
       " '   #####  Group 1 : 367 points: 200B 167M (January 1989)',\n",
       " '   #####  Revised Jan 10, 1991: Replaced zero bare nuclei in 1080185 & 1187805',\n",
       " '   #####  Revised Nov 22,1991: Removed 765878,4,5,9,7,10,10,10,3,8,1 no record',\n",
       " '   #####                  : Removed 484201,2,7,8,8,4,3,10,3,4,1 zero epithelial',\n",
       " '   #####                  : Changed 0 to 1 in field 6 of sample 1219406',\n",
       " '   #####                  : Changed 0 to 1 in field 8 of following sample:',\n",
       " '   #####                  : 1182404,2,3,1,1,1,2,0,1,1,1',\n",
       " '',\n",
       " '5. Number of Instances: 699 (as of 15 July 1992)',\n",
       " '',\n",
       " '6. Number of Attributes: 10 plus the class attribute',\n",
       " '',\n",
       " '7. Attribute Information: (class attribute has been moved to last column)',\n",
       " '',\n",
       " '   #  Attribute                     Domain',\n",
       " '   -- -----------------------------------------',\n",
       " '   1. Sample code number            id number',\n",
       " '   2. Clump Thickness               1 - 10',\n",
       " '   3. Uniformity of Cell Size       1 - 10',\n",
       " '   4. Uniformity of Cell Shape      1 - 10',\n",
       " '   5. Marginal Adhesion             1 - 10',\n",
       " '   6. Single Epithelial Cell Size   1 - 10',\n",
       " '   7. Bare Nuclei                   1 - 10',\n",
       " '   8. Bland Chromatin               1 - 10',\n",
       " '   9. Normal Nucleoli               1 - 10',\n",
       " '  10. Mitoses                       1 - 10',\n",
       " '  11. Class:                        (2 for benign, 4 for malignant)',\n",
       " '',\n",
       " '8. Missing attribute values: 16',\n",
       " '',\n",
       " '   There are 16 instances in Groups 1 to 6 that contain a single missing ',\n",
       " '   (i.e., unavailable) attribute value, now denoted by \"?\".  ',\n",
       " '',\n",
       " '9. Class distribution:',\n",
       " ' ',\n",
       " '   Benign: 458 (65.5%)',\n",
       " '   Malignant: 241 (34.5%)']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relevant for us information is at **7.Attribute Information**. It contains the names of the features and a brief description of their meaning and/or values they can take. \n",
    "\n",
    "**Q4. The next task is quite challening and may involve quite a few steps. Using the file you imported, which contains the names, extract the names of the features. In this case, the file is not so complicated but do not just copy the features names to a new list. Make sure you use your Python coding skills to extract the relevant information. In the end, we want the names of the features in a list.** \n",
    "\n",
    "    NOTE: This is a task which can be solved in many different ways. It is likely to take a few lines of code. Utilize what you have learned so far about lists, list slicing, string splitting and cleaning, and/or joining. If you are familiar with regular expressions (regex), you can utilize them as well. My solution does not make use of regex, only material we have covered so far. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "117\n"
     ]
    }
   ],
   "source": [
    "print(lines.index('7. Attribute Information: (class attribute has been moved to last column)'))\n",
    "print(lines.index('8. Missing attribute values: 16'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['   1. Sample code number            id number',\n",
       " '   2. Clump Thickness               1 - 10',\n",
       " '   3. Uniformity of Cell Size       1 - 10',\n",
       " '   4. Uniformity of Cell Shape      1 - 10',\n",
       " '   5. Marginal Adhesion             1 - 10',\n",
       " '   6. Single Epithelial Cell Size   1 - 10',\n",
       " '   7. Bare Nuclei                   1 - 10',\n",
       " '   8. Bland Chromatin               1 - 10',\n",
       " '   9. Normal Nucleoli               1 - 10',\n",
       " '  10. Mitoses                       1 - 10',\n",
       " '  11. Class:                        (2 for benign, 4 for malignant)']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted = lines[105:116]\n",
    "extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1.', 'Sample', 'code', 'number', 'id', 'number'], ['2.', 'Clump', 'Thickness', '1', '-', '10'], ['3.', 'Uniformity', 'of', 'Cell', 'Size', '1', '-', '10'], ['4.', 'Uniformity', 'of', 'Cell', 'Shape', '1', '-', '10'], ['5.', 'Marginal', 'Adhesion', '1', '-', '10'], ['6.', 'Single', 'Epithelial', 'Cell', 'Size', '1', '-', '10'], ['7.', 'Bare', 'Nuclei', '1', '-', '10'], ['8.', 'Bland', 'Chromatin', '1', '-', '10'], ['9.', 'Normal', 'Nucleoli', '1', '-', '10'], ['10.', 'Mitoses', '1', '-', '10'], ['11.', 'Class', '(2', 'for', 'benign,', '4', 'for', 'malignant)']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Sample', 'code', 'number', 'id', 'number'],\n",
       " ['Clump', 'Thickness'],\n",
       " ['Uniformity', 'of', 'Cell', 'Size'],\n",
       " ['Uniformity', 'of', 'Cell', 'Shape'],\n",
       " ['Marginal', 'Adhesion'],\n",
       " ['Single', 'Epithelial', 'Cell', 'Size'],\n",
       " ['Bare', 'Nuclei'],\n",
       " ['Bland', 'Chromatin'],\n",
       " ['Normal', 'Nucleoli'],\n",
       " ['Mitoses'],\n",
       " ['Class', 'for', 'for']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_split = [line.replace(':', '').split() for line in extracted]\n",
    "print(extracted_split)\n",
    "extracted_split_no_digits = [[item for item in line if item.isalpha()]for line in extracted_split]\n",
    "extracted_split_no_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Sample', 'code', 'number'], ['Clump', 'Thickness'], ['Uniformity', 'of', 'Cell', 'Size'], ['Uniformity', 'of', 'Cell', 'Shape'], ['Marginal', 'Adhesion'], ['Single', 'Epithelial', 'Cell', 'Size'], ['Bare', 'Nuclei'], ['Bland', 'Chromatin'], ['Normal', 'Nucleoli'], ['Mitoses'], 'Class']\n"
     ]
    }
   ],
   "source": [
    "# Remove the last two items from first and last lines \n",
    "extracted_split_no_digits[0] = extracted_split_no_digits[0][:3]\n",
    "extracted_split_no_digits[-1] = extracted_split_no_digits[-1][0]\n",
    "print(extracted_split_no_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "newlines = []\n",
    "for line in extracted_split_no_digits[:-1]:\n",
    "    #for item in line:\n",
    "    newlines.append('_'.join(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sample_code_number', 'Clump_Thickness', 'Uniformity_of_Cell_Size', 'Uniformity_of_Cell_Shape', 'Marginal_Adhesion', 'Single_Epithelial_Cell_Size', 'Bare_Nuclei', 'Bland_Chromatin', 'Normal_Nucleoli', 'Mitoses', 'Class']\n"
     ]
    }
   ],
   "source": [
    "newlines.append('Class')\n",
    "print(newlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have the names of the columns in a list called 'newlines'. We can assign it to the DataFrame we imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample_code_number</th>\n",
       "      <th>Clump_Thickness</th>\n",
       "      <th>Uniformity_of_Cell_Size</th>\n",
       "      <th>Uniformity_of_Cell_Shape</th>\n",
       "      <th>Marginal_Adhesion</th>\n",
       "      <th>Single_Epithelial_Cell_Size</th>\n",
       "      <th>Bare_Nuclei</th>\n",
       "      <th>Bland_Chromatin</th>\n",
       "      <th>Normal_Nucleoli</th>\n",
       "      <th>Mitoses</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000025</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002945</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1015425</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1016277</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1017023</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sample_code_number  Clump_Thickness  Uniformity_of_Cell_Size  \\\n",
       "0             1000025                5                        1   \n",
       "1             1002945                5                        4   \n",
       "2             1015425                3                        1   \n",
       "3             1016277                6                        8   \n",
       "4             1017023                4                        1   \n",
       "\n",
       "   Uniformity_of_Cell_Shape  Marginal_Adhesion  Single_Epithelial_Cell_Size  \\\n",
       "0                         1                  1                            2   \n",
       "1                         4                  5                            7   \n",
       "2                         1                  1                            2   \n",
       "3                         8                  1                            3   \n",
       "4                         1                  3                            2   \n",
       "\n",
       "  Bare_Nuclei  Bland_Chromatin  Normal_Nucleoli  Mitoses  Class  \n",
       "0           1                3                1        1      2  \n",
       "1          10                3                2        1      2  \n",
       "2           2                3                1        1      2  \n",
       "3           4                3                7        1      2  \n",
       "4           1                3                1        1      2  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc.columns = newlines\n",
    "bc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sample code number',\n",
       " 'Clump Thickness',\n",
       " 'Uniformity of Cell Size',\n",
       " 'Uniformity of Cell Shape',\n",
       " 'Marginal Adhesion',\n",
       " 'Single Epithelial Cell Size',\n",
       " 'Bare Nuclei',\n",
       " 'Bland Chromatin',\n",
       " 'Normal Nucleoli',\n",
       " 'Mitoses',\n",
       " 'Class']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_1 = [x.split(\".\")[1] for x in extracted]\n",
    "step_2 = [x.replace(':', '').split(\"  \")[0].strip() for x in step_1]\n",
    "step_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract from the web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting data from the web is also referred to as scraping. There are many use cases in data science where you may want to do that. Usually, you'd like to enrich your data with some extra information. Say we are building a house price forecasting model, then getting some extra information about the area/neighborhood could increase your predictive power. \n",
    "\n",
    "However, not all website can be scraped freely. Some website explicitly forbid automated scraping. There could be good/valid reasons for that. \n",
    "1. The site has good reasons to protect its data (i.e. the data there is not publicly available, and could be sensitive/proprietry).\n",
    "2. Making many and repeated requests to a website may use too much bandwidth, slowing down the website/service for other users. \n",
    "\n",
    "**Always check the website's polcity regarding scraping before you go ahead and try to scrape it!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first package we will work with is built-in the standard Python stack and is called *urllib*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen, Request\n",
    "\n",
    "# Define the url\n",
    "url = 'https://www.theguardian.com/international'\n",
    "# Send a request to it\n",
    "request = Request(url)\n",
    "# Catches the response, returning an HTTPResponse object\n",
    "response = urlopen(request) \n",
    "# Use the .read() method of the object\n",
    "html = response.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<http.client.HTTPResponse at 0x2e601390fd0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see that when we read the response, we get sequence of bytes\n",
    "#html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Another solution, without using Request (saving a line of code):\n",
    "url = 'https://www.theguardian.com/international'\n",
    "page = urlopen(url)\n",
    "html2 = page.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(html2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can use .decode() to decode the bytes to a string, using a 'utf-8' encoding\n",
    "html2 = html2.decode('utf-8')\n",
    "type(html2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract from the web using Requests package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requests package is another way to do this. It's a super popular package, which does the same but provides higher-level interfact, so requires us to write fewer lines of code. \n",
    "\n",
    "Most companies out there use the **requests** package to extract data from the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://www.theguardian.com/international'\n",
    "r = requests.get(url)\n",
    "text_output = r.text # we apply the .text method, which return the html as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML parser: BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the result is the same in both cases : a string object, extracting the html of a web page. It can be a bit difficult to work with as it contains a lot of structured and unstructured data. You can create/extract structure from it either manually, using for example regular expressions.\n",
    "\n",
    "Alterntatively, you can use a parser, which is a package that can do that for you. Enter **beautifulsoup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the package is you don't have it:\n",
    "\n",
    "#!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create an BS4 object, parsing the html object and what type of parse Python should employ \n",
    "soup = BeautifulSoup(text_output, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(soup.prettify())  # <==> the same output as print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real web pages are quite complex and could be very long. I am therefore commenting out the object. We can however, select any tag within it (where a tag is an object between < name > and whose end is identified as < /name >.\n",
    "\n",
    "Let's select all the buttons for example. Note that the result will always be a list. \n",
    "\n",
    "Another note: one of the main difficulties with scraping tasks (especially if one is not very familiar with html and css) is to figure out what to pass to the select method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<button aria-expanded=\"true\" aria-haspopup=\"true\" class=\"menu-item__title menu-item__title--News hide-from-desktop js-navigation-toggle\" data-link-name=\"nav2 : secondary : News\" role=\"menuitem\">\n",
       " <i class=\"menu-item__toggle\"></i>\n",
       " News\n",
       " </button>,\n",
       " <button aria-expanded=\"true\" aria-haspopup=\"true\" class=\"menu-item__title menu-item__title--Opinion hide-from-desktop js-navigation-toggle\" data-link-name=\"nav2 : secondary : Opinion\" role=\"menuitem\">\n",
       " <i class=\"menu-item__toggle\"></i>\n",
       " Opinion\n",
       " </button>,\n",
       " <button aria-expanded=\"true\" aria-haspopup=\"true\" class=\"menu-item__title menu-item__title--Sport hide-from-desktop js-navigation-toggle\" data-link-name=\"nav2 : secondary : Sport\" role=\"menuitem\">\n",
       " <i class=\"menu-item__toggle\"></i>\n",
       " Sport\n",
       " </button>,\n",
       " <button aria-expanded=\"true\" aria-haspopup=\"true\" class=\"menu-item__title menu-item__title--Culture hide-from-desktop js-navigation-toggle\" data-link-name=\"nav2 : secondary : Culture\" role=\"menuitem\">\n",
       " <i class=\"menu-item__toggle\"></i>\n",
       " Culture\n",
       " </button>,\n",
       " <button aria-expanded=\"true\" aria-haspopup=\"true\" class=\"menu-item__title menu-item__title--Lifestyle hide-from-desktop js-navigation-toggle\" data-link-name=\"nav2 : secondary : Lifestyle\" role=\"menuitem\">\n",
       " <i class=\"menu-item__toggle\"></i>\n",
       " Lifestyle\n",
       " </button>,\n",
       " <button class=\"menu-search__submit\" data-link-name=\"nav2 : search : submit\" for=\"submit-google-search\" type=\"submit\">\n",
       " <i class=\"right-arrow__icon\"></i>\n",
       " <span class=\"u-h\">Search with google</span>\n",
       " </button>,\n",
       " <button aria-expanded=\"true\" aria-haspopup=\"true\" class=\"menu-item__title js-navigation-toggle\" data-link-name=\"nav2 : edition picker\">\n",
       " <i class=\"menu-item__toggle\"></i>\n",
       " International edition\n",
       " </button>,\n",
       " <button class=\"subnav-link subnav-link--toggle-more js-toggle-more-sections\" data-link-name=\"nav2 : subnav-toggle\">\n",
       " More\n",
       " </button>,\n",
       " <button class=\"button button--medium button--primary button--show-more collection__show-more js-show-more-button modern-visible\" data-link-name=\"more\" data-test-id=\"show-more\"> <span class=\"inline-plus inline-icon\">\n",
       " <svg class=\"inline-plus__svg inline-icon__svg\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\">\n",
       " <path d=\"M8.2 0h1.6l.4 7.8 7.8.4v1.6l-7.8.4-.4 7.8H8.2l-.4-7.8L0 9.8V8.2l7.8-.4.4-7.8z\"></path>\n",
       " </svg> </span> <span class=\"inline-minus inline-icon\">\n",
       " <svg class=\"inline-minus__svg inline-icon__svg\" height=\"32\" viewbox=\"0 0 32 32\" width=\"32\">\n",
       " <path d=\"M5 15h22v3H5z\"></path>\n",
       " </svg> </span> <span class=\"js-button-text\">More Opinion</span> </button>,\n",
       " <button class=\"button button--medium button--primary button--show-more collection__show-more js-show-more-button modern-visible\" data-link-name=\"more\" data-test-id=\"show-more\"> <span class=\"inline-plus inline-icon\">\n",
       " <svg class=\"inline-plus__svg inline-icon__svg\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\">\n",
       " <path d=\"M8.2 0h1.6l.4 7.8 7.8.4v1.6l-7.8.4-.4 7.8H8.2l-.4-7.8L0 9.8V8.2l7.8-.4.4-7.8z\"></path>\n",
       " </svg> </span> <span class=\"inline-minus inline-icon\">\n",
       " <svg class=\"inline-minus__svg inline-icon__svg\" height=\"32\" viewbox=\"0 0 32 32\" width=\"32\">\n",
       " <path d=\"M5 15h22v3H5z\"></path>\n",
       " </svg> </span> <span class=\"js-button-text\">More Sport</span> </button>,\n",
       " <button class=\"button button--medium button--primary button--show-more collection__show-more js-show-more-button modern-visible\" data-link-name=\"more\" data-test-id=\"show-more\"> <span class=\"inline-plus inline-icon\">\n",
       " <svg class=\"inline-plus__svg inline-icon__svg\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\">\n",
       " <path d=\"M8.2 0h1.6l.4 7.8 7.8.4v1.6l-7.8.4-.4 7.8H8.2l-.4-7.8L0 9.8V8.2l7.8-.4.4-7.8z\"></path>\n",
       " </svg> </span> <span class=\"inline-minus inline-icon\">\n",
       " <svg class=\"inline-minus__svg inline-icon__svg\" height=\"32\" viewbox=\"0 0 32 32\" width=\"32\">\n",
       " <path d=\"M5 15h22v3H5z\"></path>\n",
       " </svg> </span> <span class=\"js-button-text\">More This is Europe</span> </button>,\n",
       " <button class=\"button button--medium button--primary button--show-more collection__show-more js-show-more-button modern-visible\" data-link-name=\"more\" data-test-id=\"show-more\"> <span class=\"inline-plus inline-icon\">\n",
       " <svg class=\"inline-plus__svg inline-icon__svg\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\">\n",
       " <path d=\"M8.2 0h1.6l.4 7.8 7.8.4v1.6l-7.8.4-.4 7.8H8.2l-.4-7.8L0 9.8V8.2l7.8-.4.4-7.8z\"></path>\n",
       " </svg> </span> <span class=\"inline-minus inline-icon\">\n",
       " <svg class=\"inline-minus__svg inline-icon__svg\" height=\"32\" viewbox=\"0 0 32 32\" width=\"32\">\n",
       " <path d=\"M5 15h22v3H5z\"></path>\n",
       " </svg> </span> <span class=\"js-button-text\">More Around the world</span> </button>,\n",
       " <button class=\"button button--medium button--primary button--show-more collection__show-more js-show-more-button modern-visible\" data-link-name=\"more\" data-test-id=\"show-more\"> <span class=\"inline-plus inline-icon\">\n",
       " <svg class=\"inline-plus__svg inline-icon__svg\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\">\n",
       " <path d=\"M8.2 0h1.6l.4 7.8 7.8.4v1.6l-7.8.4-.4 7.8H8.2l-.4-7.8L0 9.8V8.2l7.8-.4.4-7.8z\"></path>\n",
       " </svg> </span> <span class=\"inline-minus inline-icon\">\n",
       " <svg class=\"inline-minus__svg inline-icon__svg\" height=\"32\" viewbox=\"0 0 32 32\" width=\"32\">\n",
       " <path d=\"M5 15h22v3H5z\"></path>\n",
       " </svg> </span> <span class=\"js-button-text\">More Climate crisis</span> </button>,\n",
       " <button class=\"button button--medium button--primary button--show-more collection__show-more js-show-more-button modern-visible\" data-link-name=\"more\" data-test-id=\"show-more\"> <span class=\"inline-plus inline-icon\">\n",
       " <svg class=\"inline-plus__svg inline-icon__svg\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\">\n",
       " <path d=\"M8.2 0h1.6l.4 7.8 7.8.4v1.6l-7.8.4-.4 7.8H8.2l-.4-7.8L0 9.8V8.2l7.8-.4.4-7.8z\"></path>\n",
       " </svg> </span> <span class=\"inline-minus inline-icon\">\n",
       " <svg class=\"inline-minus__svg inline-icon__svg\" height=\"32\" viewbox=\"0 0 32 32\" width=\"32\">\n",
       " <path d=\"M5 15h22v3H5z\"></path>\n",
       " </svg> </span> <span class=\"js-button-text\">More Culture</span> </button>,\n",
       " <button class=\"u-button-reset paidfor-label__btn popup__toggle\" data-toggle=\"js-paidfor-popup46736be2-e870-4d43-9949-cb05451fe656\">About </button>,\n",
       " <button class=\"button button--medium button--primary button--show-more collection__show-more js-show-more-button modern-visible\" data-link-name=\"more\" data-test-id=\"show-more\"> <span class=\"inline-plus inline-icon\">\n",
       " <svg class=\"inline-plus__svg inline-icon__svg\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\">\n",
       " <path d=\"M8.2 0h1.6l.4 7.8 7.8.4v1.6l-7.8.4-.4 7.8H8.2l-.4-7.8L0 9.8V8.2l7.8-.4.4-7.8z\"></path>\n",
       " </svg> </span> <span class=\"inline-minus inline-icon\">\n",
       " <svg class=\"inline-minus__svg inline-icon__svg\" height=\"32\" viewbox=\"0 0 32 32\" width=\"32\">\n",
       " <path d=\"M5 15h22v3H5z\"></path>\n",
       " </svg> </span> <span class=\"js-button-text\">More Lifestyle</span> </button>,\n",
       " <button class=\"button button--medium button--primary button--show-more collection__show-more js-show-more-button modern-visible\" data-link-name=\"more\" data-test-id=\"show-more\"> <span class=\"inline-plus inline-icon\">\n",
       " <svg class=\"inline-plus__svg inline-icon__svg\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\">\n",
       " <path d=\"M8.2 0h1.6l.4 7.8 7.8.4v1.6l-7.8.4-.4 7.8H8.2l-.4-7.8L0 9.8V8.2l7.8-.4.4-7.8z\"></path>\n",
       " </svg> </span> <span class=\"inline-minus inline-icon\">\n",
       " <svg class=\"inline-minus__svg inline-icon__svg\" height=\"32\" viewbox=\"0 0 32 32\" width=\"32\">\n",
       " <path d=\"M5 15h22v3H5z\"></path>\n",
       " </svg> </span> <span class=\"js-button-text\">More Explore</span> </button>,\n",
       " <button class=\"button button--medium button--primary button--show-more collection__show-more js-show-more-button modern-visible\" data-link-name=\"more\" data-test-id=\"show-more\"> <span class=\"inline-plus inline-icon\">\n",
       " <svg class=\"inline-plus__svg inline-icon__svg\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\">\n",
       " <path d=\"M8.2 0h1.6l.4 7.8 7.8.4v1.6l-7.8.4-.4 7.8H8.2l-.4-7.8L0 9.8V8.2l7.8-.4.4-7.8z\"></path>\n",
       " </svg> </span> <span class=\"inline-minus inline-icon\">\n",
       " <svg class=\"inline-minus__svg inline-icon__svg\" height=\"32\" viewbox=\"0 0 32 32\" width=\"32\">\n",
       " <path d=\"M5 15h22v3H5z\"></path>\n",
       " </svg> </span> <span class=\"js-button-text\">More In pictures</span> </button>,\n",
       " <button class=\"site-message__close-btn js-site-message-close\" data-link-name=\"hide release message\">\n",
       " <span class=\"u-h\">Close</span>\n",
       " <span class=\"inline-close inline-icon inline-close--small\">\n",
       " <svg class=\"inline-close--small__svg inline-close__svg inline-icon__svg\" height=\"18\" viewbox=\"0 0 18 18\" width=\"18\">\n",
       " <path d=\"M7.5 9L1 2l1-1 7 6.5L16 1l1 1-6.5 7 6.5 7-1 1-7-6.5L2 17l-1-1 6.5-7z\"></path>\n",
       " </svg>\n",
       " </span>\n",
       " </button>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.select('button')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: extract the button about Lifestyle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nLifestyle\\n'"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Your code goes here\n",
    "\n",
    "soup.select('button')[4].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Extract and print out the names of all the buttons, removing new line characters!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News\n",
      "Opinion\n",
      "Sport\n",
      "Culture\n",
      "Lifestyle\n",
      "Search with google\n",
      "International edition\n",
      "More\n",
      "     More Opinion \n",
      "     More Sport \n",
      "     More This is Europe \n",
      "     More Climate crisis \n",
      "     More Culture \n",
      "About \n",
      "     More Lifestyle \n",
      "     More Explore \n",
      "     More In pictures \n",
      "Close\n"
     ]
    }
   ],
   "source": [
    "## Your code goes here\n",
    "\n",
    "for item in soup.select('button'):\n",
    "    print(item.text.replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>News, sport and opinion from the Guardian's global edition | The Guardian</title>"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title# extract the title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soup.get_text() # extract the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, you only need to extract particularly information from a url, such as images or hyperlinks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soup.find_all('img')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a list, and we can unpack it a little bit. Each item in the list is a Tag object. Let's check its type and extract the first one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.Tag"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(soup.find_all('img')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<img alt=\"\" class=\"responsive-img\" loading=\"lazy\" src=\"https://i.guim.co.uk/img/media/69c583a21604059f50de525369a1749aa1c754a5/0_9_3500_2102/master/3500.jpg?width=700&amp;quality=85&amp;auto=format&amp;fit=max&amp;s=f355341ca002b8a12c6bf4b3515ed2fe\"/>"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image1 = soup.find_all('img')[0]\n",
    "image1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://i.guim.co.uk/img/media/69c583a21604059f50de525369a1749aa1c754a5/0_9_3500_2102/master/3500.jpg?width=700&quality=85&auto=format&fit=max&s=f355341ca002b8a12c6bf4b3515ed2fe'"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can access the source attribute, stores in src \n",
    "image1['src']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all images can be downloaded. If you open the source, you will get an 'Unauthorized' error. Let's ensure that is indeed the case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\"\\n \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\">\\n<html>\\n  <head>\\n    <title>401 Unauthorized - missing signature</title>\\n  </head>\\n  <body>\\n    <h1>Error 401 Unauthorized - missing signature</h1>\\n    <p>Unauthorized - missing signature</p>\\n    <h3>Guru Mediation:</h3>\\n    <p>Details: cache-ams21078-AMS 1610208940 1702990014</p>\\n    <hr>\\n    <p>Varnish cache server</p>\\n  </body>\\n</html>\\n'"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = requests.get('https://i.guim.co.uk/img/media/69c583a21604059f50de525369a1749aa1c754a5/0_9_3500_2102/master/3500.jpg?width=700&amp;quality=85&amp;auto=format&amp;fit=max&amp;s=f355341ca002b8a12c6bf4b3515ed2fe')\n",
    "\n",
    "image.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take-home challenge(not graded or requested to be submitted): Go to a https://books.toscrape.com/ and scrape it. Return the title of every book with 2-star rating (not >= 2 **, only those books with exactly 2 stars).** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Note that the books are not all listed on one page but across multiple pages. \n",
    "\n",
    "base_url = 'https://books.toscrape.com/catalogue/page-{}.html'\n",
    "\n",
    "# I inspected a few items and see that i need to grab a class = 'star-rating Two', which is a part of the 'product_pod' class.\n",
    "# Get for the first page\n",
    "\n",
    "res = requests.get(base_url.format(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's select the class call for product_pod; if you check its length, it will be 20 since we have 20 books per page\n",
    "products = soup.select('.product_pod')\n",
    "len(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<article class=\"product_pod\">\n",
      "<div class=\"image_container\">\n",
      "<a href=\"a-light-in-the-attic_1000/index.html\"><img alt=\"A Light in the Attic\" class=\"thumbnail\" src=\"../media/cache/2c/da/2cdad67c44b002e7ead0cc35693c0e8b.jpg\"/></a>\n",
      "</div>\n",
      "<p class=\"star-rating Three\">\n",
      "<i class=\"icon-star\"></i>\n",
      "<i class=\"icon-star\"></i>\n",
      "<i class=\"icon-star\"></i>\n",
      "<i class=\"icon-star\"></i>\n",
      "<i class=\"icon-star\"></i>\n",
      "</p>\n",
      "<h3><a href=\"a-light-in-the-attic_1000/index.html\" title=\"A Light in the Attic\">A Light in the ...</a></h3>\n",
      "<div class=\"product_price\">\n",
      "<p class=\"price_color\">Â£51.77</p>\n",
      "<p class=\"instock availability\">\n",
      "<i class=\"icon-ok\"></i>\n",
      "    \n",
      "        In stock\n",
      "    \n",
      "</p>\n",
      "<form>\n",
      "<button class=\"btn btn-primary btn-block\" data-loading-text=\"Adding...\" type=\"submit\">Add to basket</button>\n",
      "</form>\n",
      "</div>\n",
      "</article>\n"
     ]
    }
   ],
   "source": [
    "# Now we want to grab the title for 2-star rated books, let's first do it for the first book: \n",
    "\n",
    "example = products[0]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"star-rating Three\">\n",
       " <i class=\"icon-star\"></i>\n",
       " <i class=\"icon-star\"></i>\n",
       " <i class=\"icon-star\"></i>\n",
       " <i class=\"icon-star\"></i>\n",
       " <i class=\"icon-star\"></i>\n",
       " </p>]"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We see this book has 3-stars as a rating; let's check if that is the case by grabbin the class\n",
    "example.select('.star-rating.Three')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"a-light-in-the-attic_1000/index.html\"><img alt=\"A Light in the Attic\" class=\"thumbnail\" src=\"../media/cache/2c/da/2cdad67c44b002e7ead0cc35693c0e8b.jpg\"/></a>,\n",
       " <a href=\"a-light-in-the-attic_1000/index.html\" title=\"A Light in the Attic\">A Light in the ...</a>]"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How can we grab the title? We see it in the linking element 'a'\n",
    "example.select('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"a-light-in-the-attic_1000/index.html\" title=\"A Light in the Attic\">A Light in the ...</a>\n",
      "A Light in the Attic\n"
     ]
    }
   ],
   "source": [
    "# First result is image, the second is the title\n",
    "print(example.select('a')[1])\n",
    "\n",
    "# Now we need to grab the title of this result; we call ['title']\n",
    "print(example.select('a')[1]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to do this for all books in the page, then all pages\n",
    "\n",
    "two_star_titles = []\n",
    "\n",
    "for n in range(1, 51):\n",
    "    scrape_url = base_url.format(n)\n",
    "    res = requests.get(scrape_url)\n",
    "    \n",
    "    soup = BeautifulSoup(res.text, 'lxml')\n",
    "    books = soup.select('.product_pod')\n",
    "    \n",
    "    for book in books:\n",
    "        # If the list is not empty, then I have a 2-stars book\n",
    "        if len(book.select('.star-rating.Two')) != 0: \n",
    "            book_title = book.select('a')[1]['title']\n",
    "            two_star_titles.append(book_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(two_star_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with HTML forms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to scrape a website using a specific query, then **BeautifulSoup** may not get us very far (though it is still a very useful package). \n",
    "\n",
    "When you need to interact with a website (click a button,for example), then we need another approach. One package that allows us to do that is **MechanicalSoup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting MechanicalSoup\n",
      "  Downloading MechanicalSoup-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: lxml in c:\\users\\mishe\\anaconda3\\lib\\site-packages (from MechanicalSoup) (4.5.2)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4 in c:\\users\\mishe\\anaconda3\\lib\\site-packages (from MechanicalSoup) (4.9.1)\n",
      "Requirement already satisfied: requests>=2.0 in c:\\users\\mishe\\anaconda3\\lib\\site-packages (from MechanicalSoup) (2.24.0)\n",
      "Requirement already satisfied: six>=1.4 in c:\\users\\mishe\\anaconda3\\lib\\site-packages (from MechanicalSoup) (1.15.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\mishe\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.4->MechanicalSoup) (2.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mishe\\anaconda3\\lib\\site-packages (from requests>=2.0->MechanicalSoup) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\mishe\\anaconda3\\lib\\site-packages (from requests>=2.0->MechanicalSoup) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\mishe\\anaconda3\\lib\\site-packages (from requests>=2.0->MechanicalSoup) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\mishe\\anaconda3\\lib\\site-packages (from requests>=2.0->MechanicalSoup) (2.10)\n",
      "Installing collected packages: MechanicalSoup\n",
      "Successfully installed MechanicalSoup-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install MechanicalSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mechanicalsoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first create a Browser object, which represents a headless web browser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = mechanicalsoup.Browser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we specify the url and request it using the .get(url). Let's work with a simple url, which is a login page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://olympus.realpython.org/login'\n",
    "page = browser.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, a Response object; the number [200] corresponds to the status code returned by the request() where 200 means successful request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MechanicalSoup uses BeautifulSoup to parse the HTML object, as we can see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(page.soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html>\n",
       "<head>\n",
       "<title>Log In</title>\n",
       "</head>\n",
       "<body bgcolor=\"yellow\">\n",
       "<center>\n",
       "<br/><br/>\n",
       "<h2>Please log in to access Mount Olympus:</h2>\n",
       "<br/><br/>\n",
       "<form action=\"/login\" method=\"post\" name=\"login\">\n",
       "Username: <input name=\"user\" type=\"text\"/><br/>\n",
       "Password: <input name=\"pwd\" type=\"password\"/><br/><br/>\n",
       "<input type=\"submit\" value=\"Submit\"/>\n",
       "</form>\n",
       "</center>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the page has a < form > with inputs being 'Username' and 'Password'. The correct usename is 'zeus' and password is 'ThunderDude'. If you manually enter them in the page, you will be directed to the 'Profiles' page. \n",
    "\n",
    "Note from above that the < form > has its **name** attribute set to *login*, the first *input* element is the 'Username', the second is the 'Password' and the third the 'Submit' button. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.Tag'>\n",
      "<form action=\"/login\" method=\"post\" name=\"login\">\n",
      "Username: <input name=\"user\" type=\"text\" value=\"zeus\"/><br/>\n",
      "Password: <input name=\"pwd\" type=\"password\" value=\"ThunderDude\"/><br/><br/>\n",
      "<input type=\"submit\" value=\"Submit\"/>\n",
      "</form>\n"
     ]
    }
   ],
   "source": [
    "# We save the soup object as html and extract the 'form'.\n",
    "\n",
    "html = page.soup\n",
    "form = html.select(\"form\")[0] #we add the 0 to extract the Tag from a list form\n",
    "print(type(form))\n",
    "print(form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://olympus.realpython.org/profiles'"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the username input (0) and password input (1), and assign to them the correct values\n",
    "form.select(\"input\")[0][\"value\"] = \"zeus\"\n",
    "form.select(\"input\")[1][\"value\"] = \"ThunderDude\"\n",
    "\n",
    "\n",
    "# Submit the form using the 'SUBMIT' button, passing the form object and the url of the login page\n",
    "profiles_page = browser.submit(form, page.url)\n",
    "# To confirm we successfully logged in\n",
    "profiles_page.url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word of caution: hackers can use similar approach to brute force logins by traying many different combinations.\n",
    "\n",
    "**Do not try this at home!** as this is highly illegal, and many websites will lock you out after too many failed requests and block your IP address."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take-home challenge 2(again, for your own practice, no grading or submission!): Pick a website for which you have a registration and try to replicate that exercise from above. In other words, use MechanicalSoup to get a response from your url of choice and then find the appropriate log-in/sign-in form and enter your credentials to log in.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Your code goes here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
